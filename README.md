Данный проект обучает Resnet18 на классификацию жестов на открытом датасете Hagrid и использует обученную модель (+трекинг с помощью MediaPipe) для упрощения процесса управления видео или аудиоплеерами  
Жесты из Hagrid Dataset:
![gestures](https://github.com/user-attachments/assets/7def428d-6ba6-4ebb-b5a1-6e6c12cca917)

Изначально проект предполагался в виде расширения для браузера для управления плеером ютуба жестами.
Предполагаемые жесты:
1. "timeout" - включение/выключение распознавания жестов (применения функций)
2. "stop" - пауза/возобновление видео (нажимается пробел)
3. "palm" / "grabbing" + движение вправо/влево - перемотка (нажимаются стрелочки вправо/влево)
4. "two up" / "two up inverted" - быстрая перемотка (трижды нажимаются стрелочки вправо/влево)
5. "thumb index" + движение вверх/вниз - изменение громкости (volumeup / volumedown)
6. "mute" - выключить звук в системе (volumemute)
7. "gun" ("three gun") - следующее видео (нажимается комбинация shift + n)
8. "like" - поставить лайк (не реализовано, проще всего будет реализовывать для расширения для браузера)
9. "dislike" - поставить дизлайк (не реализовано, проще всего будет реализовывать для расширения для браузера)

Что было сделано:
1. Скачал датасет Hagrid. Разбил данные на train/val/test с помощью файлов photos_separation_ver2.py  
(photos_separation.py отбирает лишь некоторые жесты для обучения и некоторый процент (6) остальных кидает в no_gestures, - данное разделение не оправдало надежды).
2. Обучил Resnet18 на полученных данных.
3. Прикрутил MediaPipeLine для трекинга центра кисти для реализации перемотки/изменения громкости
4. В проде (exefile/gesture_controller_gui.py):  
4.1. Каждому кадру присваивается один из возможных жестов или "no_gesture"  
4.2. "no_gesture" присваивается также, если модель уверена в остальных жестах менее, чем на conf_threshold  
4.3. Используются n_frames кадров, которые голосуют за распознавание жеста. Результат - gesture  
4.4. Действия, привязанные к жестам, выполняются, только если жест "timeout" переводит флажок activated в статус True  
4.5. Изменение громкости и перемотка активируются, только если изменение координаты центра кисти > 5 пикселей (th_x, th_y)  
4.6. Используются ещё несколько крутых фич

В итоге было собрано приложение в виде exe файла.

Проект собирался на Python 3.10, Windows 11. Конфликт PyInstaller и Python 3.10 решался с помощью ответа MiguelMLR9 https://github.com/pyinstaller/pyinstaller/issues/6301.
Для сборки проекта использовать команду pyinstaller build.spec из папки exefile, предварительно скопировав файл с весами модели .pth в папку exefile.

Библиотеки, необходимые для сборки и работы exe-файла содержатся в exefile/reqs.txt

Для запуска приложения нужно 
1. Скачать архив собранного проекта по ссылке https://drive.google.com/file/d/1XjJH1YJq7yKSLrcHPRxucCsBm_WteLKY/view?usp=drive_link, 
2. Разархивировать (проект всего весит около 700мб+),
3. Запустить YoutubeGesturesApp.exe

Интерфейс приложения:
![image](https://github.com/user-attachments/assets/b52f9635-efe7-4e1a-886e-862e5b05f6bf)


В будущем можно/нужно реализовать (ToDo):
1. Нормальный интерфейс
2. Переназначение реализованных функций на другие жесты по желанию пользователя, 
3. Предварительное руководство/обучение при запуске приложения,
4. Уменьшить размер проекта.
5. Иногда бывает, что "gun" не нажимает комбинацию клавиш, а только шифт (bug fix)
